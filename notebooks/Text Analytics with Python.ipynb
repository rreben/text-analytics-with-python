{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics with Python\n",
    "## Text Summarization\n",
    "\n",
    "In order to be able to run the code from a jupyter notebook, we have to \"inline\" the contraction notebook in a cell.\n",
    "\n",
    "Pls check:\n",
    "I used `vagrant ssh` and `sudo -i pip install htmlparser` is this really necessary, when you change to html.parser in python 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 01 01:11:02 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4c3d0a095237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopwords\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Aug 26 20:45:10 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "# not as an import to get a clean install from basket4py\n",
    "# from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# changed from HTMLParser to html.parser (because basic changes from python 2 to python 3)\n",
    "from html.parser import HTMLParser\n",
    "import unicodedata\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "html_parser = HTMLParser()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "    \n",
    "#this seems not to be necessary and is not compatible with python 3. The pattern module is not supported by python 3.    \n",
    "#from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "    \n",
    "# lemmatize text based on POS tags    \n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "    \n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "    \n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def unescape_html(parser, text):\n",
    "    \n",
    "    return parser.unescape(text)\n",
    "\n",
    "def normalize_corpus(corpus, lemmatize=True, tokenize=False):\n",
    "    \n",
    "    normalized_corpus = []  \n",
    "    for text in corpus:\n",
    "        text = html_parser.unescape(text)\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        if lemmatize:\n",
    "            text = lemmatize_text(text)\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "        else:\n",
    "            normalized_corpus.append(text)\n",
    "            \n",
    "    return normalized_corpus\n",
    "\n",
    "\n",
    "def parse_document(document):\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    if isinstance(document, str):\n",
    "        document = document\n",
    "    elif isinstance(document, unicode):\n",
    "        return unicodedata.normalize('NFKD', document).encode('ascii', 'ignore')\n",
    "    else:\n",
    "        raise ValueError('Document is not string or unicode!')\n",
    "    document = document.strip()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Sep 03 12:38:45 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def build_feature_matrix(documents, feature_type='frequency'):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=1, \n",
    "                                     ngram_range=(1, 1))\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=1, \n",
    "                                     ngram_range=(1, 1))\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=1, \n",
    "                                     ngram_range=(1, 1))\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "    return vectorizer, feature_matrix\n",
    "\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "    \n",
    "def low_rank_svd(matrix, singular_count=2):\n",
    "    \n",
    "    u, s, vt = svds(matrix, k=singular_count)\n",
    "    return u, s, vt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Sep 04 15:24:26 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "# We \"inlined\" the modules in the cells above, so we do not have to / must not import them here\n",
    "# from normalization import normalize_corpus, parse_document\n",
    "# from utils import build_feature_matrix, low_rank_svd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_text = \"\"\"\n",
    "Elephants are large mammals of the family Elephantidae \n",
    "and the order Proboscidea. Two species are traditionally recognised, \n",
    "the African elephant and the Asian elephant. Elephants are scattered \n",
    "throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male \n",
    "African elephants are the largest extant terrestrial animals. All \n",
    "elephants have a long trunk used for many purposes, \n",
    "particularly breathing, lifting water and grasping objects. Their \n",
    "incisors grow into tusks, which can serve as weapons and as tools \n",
    "for moving objects and digging. Elephants' large ear flaps help \n",
    "to control their body temperature. Their pillar-like legs can \n",
    "carry their great weight. African elephants have larger ears \n",
    "and concave backs while Asian elephants have smaller ears \n",
    "and convex or level backs.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install `sudo -i conda install gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vagrant/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated document summarization with the gensim summarization module\n",
    "We will now leverage gensims summarization module to get an automated document summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords\n",
    "\n",
    "def text_summarization_gensim(text, summary_ratio=0.5):\n",
    "    \n",
    "    summary = summarize(text, split=True, ratio=summary_ratio)\n",
    "    for sentence in summary:\n",
    "        print (sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove newlines and tokenize for sentences. Get a list of sentences of the text.\n",
    "\n",
    "Then join the sentences back together with a space between the sentences. The text is now formatted for gensims summarization module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elephants are large mammals of the family Elephantidae  and the order Proboscidea.', 'Two species are traditionally recognised,  the African elephant and the Asian elephant.', 'Elephants are scattered  throughout sub-Saharan Africa, South Asia, and Southeast Asia.', 'Male  African elephants are the largest extant terrestrial animals.', 'All  elephants have a long trunk used for many purposes,  particularly breathing, lifting water and grasping objects.', 'Their  incisors grow into tusks, which can serve as weapons and as tools  for moving objects and digging.', \"Elephants' large ear flaps help  to control their body temperature.\", 'Their pillar-like legs can  carry their great weight.', 'African elephants have larger ears  and concave backs while Asian elephants have smaller ears  and convex or level backs.']\n",
      "Elephants are large mammals of the family Elephantidae  and the order Proboscidea. Two species are traditionally recognised,  the African elephant and the Asian elephant. Elephants are scattered  throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male  African elephants are the largest extant terrestrial animals. All  elephants have a long trunk used for many purposes,  particularly breathing, lifting water and grasping objects. Their  incisors grow into tusks, which can serve as weapons and as tools  for moving objects and digging. Elephants' large ear flaps help  to control their body temperature. Their pillar-like legs can  carry their great weight. African elephants have larger ears  and concave backs while Asian elephants have smaller ears  and convex or level backs.\n"
     ]
    }
   ],
   "source": [
    "docs = parse_document(toy_text)\n",
    "print (docs)\n",
    "text = ' '.join(docs)\n",
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two species are traditionally recognised,  the African elephant and the Asian elephant.\n",
      "All  elephants have a long trunk used for many purposes,  particularly breathing, lifting water and grasping objects.\n",
      "African elephants have larger ears  and concave backs while Asian elephants have smaller ears  and convex or level backs.\n"
     ]
    }
   ],
   "source": [
    "text_summarization_gensim(text, summary_ratio=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent semantic analysis (extraction based automated summarization)\n",
    "First let us prepare our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elephants are large mammals of the family Elephantidae  and the order Proboscidea.', 'Two species are traditionally recognised,  the African elephant and the Asian elephant.', 'Elephants are scattered  throughout sub-Saharan Africa, South Asia, and Southeast Asia.', 'Male  African elephants are the largest extant terrestrial animals.', 'All  elephants have a long trunk used for many purposes,  particularly breathing, lifting water and grasping objects.', 'Their  incisors grow into tusks, which can serve as weapons and as tools  for moving objects and digging.', \"Elephants' large ear flaps help  to control their body temperature.\", 'Their pillar-like legs can  carry their great weight.', 'African elephants have larger ears  and concave backs while Asian elephants have smaller ears  and convex or level backs.']\n",
      "['elephants large mammals family elephantidae order proboscidea', 'two species traditionally recognised african elephant asian elephant', 'elephants scattered throughout sub saharan africa south asia southeast asia', 'male african elephants largest extant terrestrial animals', 'elephants long trunk used many purposes particularly breathing lifting water grasping objects', 'incisors grow tusks serve weapons tools moving objects digging', 'elephants large ear flaps help control body temperature', 'pillar like legs carry great weight', 'african elephants larger ears concave backs asian elephants smaller ears convex level backs']\n",
      "Total Sentences in Document: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Anaconda3-4.4.0-Linux-x86_64/lib/python3.6/site-packages/ipykernel_launcher.py:101: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n"
     ]
    }
   ],
   "source": [
    "sentences = parse_document(toy_text)\n",
    "print (sentences)\n",
    "\n",
    "norm_sentences = normalize_corpus(sentences,lemmatize=False) \n",
    "print (norm_sentences)\n",
    "\n",
    "total_sentences = len(norm_sentences)\n",
    "print ('Total Sentences in Document:', total_sentences)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.02  2.07  1.53  1.4   2.1   4.48  0.7   1.2   4.65]\n",
      "[4 5 8]\n",
      "All  elephants have a long trunk used for many purposes,  particularly breathing, lifting water and grasping objects.\n",
      "Their  incisors grow into tusks, which can serve as weapons and as tools  for moving objects and digging.\n",
      "African elephants have larger ears  and concave backs while Asian elephants have smaller ears  and convex or level backs.\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 3\n",
    "num_topics = 2\n",
    "\n",
    "vec, dt_matrix = build_feature_matrix(sentences, \n",
    "                                      feature_type='frequency')\n",
    "\n",
    "td_matrix = dt_matrix.transpose()\n",
    "td_matrix = td_matrix.multiply(td_matrix > 0)\n",
    "\n",
    "u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)  \n",
    "                                         \n",
    "sv_threshold = 0.5\n",
    "min_sigma_value = max(s) * sv_threshold\n",
    "s[s < min_sigma_value] = 0\n",
    "\n",
    "salience_scores = np.sqrt(np.dot(np.square(s), np.square(vt)))\n",
    "print (np.round(salience_scores, 2))\n",
    "\n",
    "top_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\n",
    "top_sentence_indices.sort()\n",
    "print (top_sentence_indices)\n",
    "\n",
    "for index in top_sentence_indices:\n",
    "    print (sentences[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "def lsa_text_summarizer(documents, num_sentences=2,\n",
    "                        num_topics=2, feature_type='frequency',\n",
    "                        sv_threshold=0.5):\n",
    "                            \n",
    "    vec, dt_matrix = build_feature_matrix(documents, \n",
    "                                          feature_type=feature_type)\n",
    "\n",
    "    td_matrix = dt_matrix.transpose()\n",
    "    td_matrix = td_matrix.multiply(td_matrix > 0)\n",
    "\n",
    "    u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)  \n",
    "    min_sigma_value = max(s) * sv_threshold\n",
    "    s[s < min_sigma_value] = 0\n",
    "    \n",
    "    salience_scores = np.sqrt(np.dot(np.square(s), np.square(vt)))\n",
    "    top_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\n",
    "    top_sentence_indices.sort()\n",
    "    \n",
    "    for index in top_sentence_indices:\n",
    "        print sentences[index]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "import networkx\n",
    "\n",
    "num_sentences = 3\n",
    "vec, dt_matrix = build_feature_matrix(norm_sentences, \n",
    "                                      feature_type='tfidf')\n",
    "similarity_matrix = (dt_matrix * dt_matrix.T)\n",
    "print np.round(similarity_matrix.todense(), 2)\n",
    "\n",
    "similarity_graph = networkx.from_scipy_sparse_matrix(similarity_matrix)\n",
    "\n",
    "networkx.draw_networkx(similarity_graph)\n",
    "\n",
    "scores = networkx.pagerank(similarity_graph)\n",
    "\n",
    "ranked_sentences = sorted(((score, index) \n",
    "                            for index, score \n",
    "                            in scores.items()), \n",
    "                          reverse=True)\n",
    "ranked_sentences\n",
    "\n",
    "top_sentence_indices = [ranked_sentences[index][1] \n",
    "                        for index in range(num_sentences)]\n",
    "top_sentence_indices.sort()\n",
    "print top_sentence_indices\n",
    "\n",
    "for index in top_sentence_indices:\n",
    "    print sentences[index]\n",
    "    \n",
    "\n",
    "def textrank_text_summarizer(documents, num_sentences=2,\n",
    "                             feature_type='frequency'):\n",
    "    \n",
    "    vec, dt_matrix = build_feature_matrix(norm_sentences, \n",
    "                                      feature_type='tfidf')\n",
    "    similarity_matrix = (dt_matrix * dt_matrix.T)\n",
    "        \n",
    "    similarity_graph = networkx.from_scipy_sparse_matrix(similarity_matrix)\n",
    "    scores = networkx.pagerank(similarity_graph)   \n",
    "    \n",
    "    ranked_sentences = sorted(((score, index) \n",
    "                                for index, score \n",
    "                                in scores.items()), \n",
    "                              reverse=True)\n",
    "\n",
    "    top_sentence_indices = [ranked_sentences[index][1] \n",
    "                            for index in range(num_sentences)]\n",
    "    top_sentence_indices.sort()\n",
    "    \n",
    "    for index in top_sentence_indices:\n",
    "        print sentences[index]                             \n",
    "    \n",
    "\n",
    "DOCUMENT = \"\"\"\n",
    "The Elder Scrolls V: Skyrim is an open world action role-playing video game \n",
    "developed by Bethesda Game Studios and published by Bethesda Softworks. \n",
    "It is the fifth installment in The Elder Scrolls series, following \n",
    "The Elder Scrolls IV: Oblivion. Skyrim's main story revolves around \n",
    "the player character and their effort to defeat Alduin the World-Eater, \n",
    "a dragon who is prophesied to destroy the world. \n",
    "The game is set two hundred years after the events of Oblivion \n",
    "and takes place in the fictional province of Skyrim. The player completes quests \n",
    "and develops the character by improving skills. \n",
    "Skyrim continues the open world tradition of its predecessors by allowing the \n",
    "player to travel anywhere in the game world at any time, and to \n",
    "ignore or postpone the main storyline indefinitely. The player may freely roam \n",
    "over the land of Skyrim, which is an open world environment consisting \n",
    "of wilderness expanses, dungeons, cities, towns, fortresses and villages. \n",
    "Players may navigate the game world more quickly by riding horses, \n",
    "or by utilizing a fast-travel system which allows them to warp to previously \n",
    "Players have the option to develop their character. At the beginning of the game, \n",
    "players create their character by selecting one of several races, \n",
    "including humans, orcs, elves and anthropomorphic cat or lizard-like creatures, \n",
    "and then customizing their character's appearance.discovered locations. Over the \n",
    "course of the game, players improve their character's skills, which are numerical \n",
    "representations of their ability in certain areas. There are eighteen skills \n",
    "divided evenly among the three schools of combat, magic, and stealth. \n",
    "Skyrim is the first entry in The Elder Scrolls to include Dragons in the game's \n",
    "wilderness. Like other creatures, Dragons are generated randomly in the world \n",
    "and will engage in combat. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sentences = parse_document(DOCUMENT)\n",
    "norm_sentences = normalize_corpus(sentences,lemmatize=True) \n",
    "print \"Total Sentences:\", len(norm_sentences) \n",
    "\n",
    "lsa_text_summarizer(norm_sentences, num_sentences=3,\n",
    "                    num_topics=5, feature_type='frequency',\n",
    "                    sv_threshold=0.5)  \n",
    "\n",
    "textrank_text_summarizer(norm_sentences, num_sentences=3,\n",
    "                         feature_type='tfidf')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
